{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e00688a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Python\\ABSA_Prompting\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\Python\\ABSA_Prompting\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n\n",
    "DATA_DIR = r\"c:\\Users\\Admin\\Python\\ABSA_Prompting\\data\"\n",
    "RESULT_DIR = r\"c:\\Users\\Admin\\Python\\ABSA_Prompting\\results\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4a836f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa5c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "ViABSA_BP_dir = path.join(DATA_DIR, 'ViABSA_Hotel')\n",
    "test_file = path.join(ViABSA_BP_dir, 'data_test.csv')\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4defe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = [\n",
    "    \"FACILITIES#CLEANLINESS\",\n",
    "    \"FACILITIES#COMFORT\",\n",
    "    \"FACILITIES#DESIGN&FEATURES\",\n",
    "    \"FACILITIES#GENERAL\",\n",
    "    \"FACILITIES#MISCELLANEOUS\",\n",
    "    \"FACILITIES#PRICES\",\n",
    "    \"FACILITIES#QUALITY\",\n",
    "    \"FOOD&DRINKS#MISCELLANEOUS\",\n",
    "    \"FOOD&DRINKS#PRICES\",\n",
    "    \"FOOD&DRINKS#QUALITY\",\n",
    "    \"FOOD&DRINKS#STYLE&OPTIONS\",\n",
    "    \"HOTEL#CLEANLINESS\",\n",
    "    \"HOTEL#COMFORT\",\n",
    "    \"HOTEL#DESIGN&FEATURES\",\n",
    "    \"HOTEL#GENERAL\",\n",
    "    \"HOTEL#MISCELLANEOUS\",\n",
    "    \"HOTEL#PRICES\",\n",
    "    \"HOTEL#QUALITY\",\n",
    "    \"LOCATION#GENERAL\",\n",
    "    \"ROOMS#CLEANLINESS\",\n",
    "    \"ROOMS#COMFORT\",\n",
    "    \"ROOMS#DESIGN&FEATURES\",\n",
    "    \"ROOMS#GENERAL\",\n",
    "    \"ROOMS#MISCELLANEOUS\",\n",
    "    \"ROOMS#PRICES\",\n",
    "    \"ROOMS#QUALITY\",\n",
    "    \"ROOM_AMENITIES#CLEANLINESS\",\n",
    "    \"ROOM_AMENITIES#COMFORT\",\n",
    "    \"ROOM_AMENITIES#DESIGN&FEATURES\",\n",
    "    \"ROOM_AMENITIES#GENERAL\",\n",
    "    \"ROOM_AMENITIES#MISCELLANEOUS\",\n",
    "    \"ROOM_AMENITIES#PRICES\",\n",
    "    \"ROOM_AMENITIES#QUALITY\",\n",
    "    \"SERVICE#GENERAL\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e858e993",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {\n",
    "    1: \"positive\",\n",
    "    2: \"negative\",\n",
    "    3: \"neutral\"\n",
    "}\n",
    "\n",
    "def transform_aspect_sentiment(df, start=0, end=None):\n",
    "    result = [] \n",
    "    \n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "\n",
    "    for idx, row in df.iloc[start:end].iterrows():\n",
    "        entry = {\n",
    "            \"id\": str(idx),\n",
    "            \"text\": row['Review'],\n",
    "            \"sentiments\": []\n",
    "        }\n",
    "\n",
    "        for aspect in aspects:\n",
    "            sentiment = row[f\"{aspect}_label\"]\n",
    "            if sentiment == 1:  # ch·ªâ l·∫•y nh·ªØng c√°i c√≥ sentiment\n",
    "                aspect_sentiment_value = row[aspect]\n",
    "                mapped_sent = sentiment_map.get(aspect_sentiment_value, \"unknown\")\n",
    "                if aspect_sentiment_value != 'none':\n",
    "                    entry[\"sentiments\"].append({\n",
    "                        \"aspect\": aspect,\n",
    "                        \"sentiment\": mapped_sent\n",
    "                    })\n",
    "                else:\n",
    "                    # n·∫øu c·ªôt sentiment text b·ªã none nh∆∞ng label == 1 th√¨ c√≥ th·ªÉ log ra ki·ªÉm tra\n",
    "                    entry[\"sentiments\"].append({\n",
    "                        \"aspect\": aspect,\n",
    "                        \"sentiment\": \"unknown\"\n",
    "                    })\n",
    "\n",
    "        result.append(entry)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6741f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0',\n",
       "  'text': 'Ga gi∆∞∆°ÃÄng kh√¥ng saÃ£ch, nh√¢n vi√™n qu√™n doÃ£n phoÃÄng m√¥Ã£t ngaÃÄy.',\n",
       "  'sentiments': [{'aspect': 'ROOM_AMENITIES#CLEANLINESS',\n",
       "    'sentiment': 'negative'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'negative'}]},\n",
       " {'id': '1',\n",
       "  'text': 'Nv nhi·ªát t√¨nh, ph√≤ng ·ªü s·∫°ch s·∫Ω, ti·ªán nghi, v·ªã tr√≠ kh√° thu·∫≠n ti·ªán cho vi·ªác di chuy·ªÉn ƒë·∫øn c√°c ƒë·ªãa ƒëi·ªÉm ƒÉn + ch∆°i Ph√≤ng c√≥ gi√°n',\n",
       "  'sentiments': [{'aspect': 'LOCATION#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'neutral'},\n",
       "   {'aspect': 'ROOMS#COMFORT', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '2',\n",
       "  'text': 'ƒêi b·ªô ra bi·ªÉn g·∫ßn, ti·ªán ƒëi l·∫°i Ph√≤ng view bi·ªÉn nh∆∞ng c·ª≠a s·ªï view bi·ªÉn kh√° b√©',\n",
       "  'sentiments': [{'aspect': 'LOCATION#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#DESIGN&FEATURES', 'sentiment': 'neutral'},\n",
       "   {'aspect': 'ROOMS#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '3',\n",
       "  'text': 'T·∫•t c·∫£ m·ªçi th·ª© ƒë·ªÅu s·∫°ch s·∫Ω, gi∆∞·ªùng ng·ªß r·∫•t tho·∫£i m√°i. Kh√¥ng c√≥ qu·∫°t ƒëi·ªán m√† ch·ªâ c√≥ ƒëi·ªÅu h√≤a n√™n c√≥ ch√∫t b·∫•t ti·ªán.',\n",
       "  'sentiments': [{'aspect': 'HOTEL#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#COMFORT', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#DESIGN&FEATURES', 'sentiment': 'negative'}]},\n",
       " {'id': '4',\n",
       "  'text': 'Gi∆∞·ªùng s·∫°ch s·∫Ω, tho·∫£i m√°i, ƒë·∫ßy ƒë·ªß thi·∫øt b·ªã c·∫ßn thi·∫øt, ƒÉn c≈©ng trang b·ªã s·∫µn n·∫øu b·∫°n l∆∞·ªùi Ph√≤ng t·∫Øm ch∆∞a chia ƒë∆∞·ª£c ko gian kh√¥ v√† ∆∞·ªõt',\n",
       "  'sentiments': [{'aspect': 'FOOD&DRINKS#STYLE&OPTIONS',\n",
       "    'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#COMFORT', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#DESIGN&FEATURES', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOM_AMENITIES#GENERAL', 'sentiment': 'positive'}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SETUP DATA\n",
    "test_df[aspects] = test_df[aspects].fillna('none')\n",
    "\n",
    "for aspect in aspects:\n",
    "    test_df[aspect + '_label'] = (test_df[aspect] != 0).astype(int)\n",
    "\n",
    "test_json = transform_aspect_sentiment(test_df, 0, 100)\n",
    "test_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41d81c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:05<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 3 diverse examples:\n",
      "\n",
      "Example 545:\n",
      "b·ªØa s√°ng ngon,nh√¢n vi√™n vui v·∫ª,t·∫≠n t√¨nh,ƒë·∫∑c bi·∫øt c√¥ b√© Ly r·∫•t chu ƒë√°o v√† t·∫≠n t√¨nh. ƒë·ªì ƒÉn s√°ng h∆°i it m√≥n,ph√≤ng t·∫Øm qu√° nh·ªè\n",
      "\n",
      "Example 302:\n",
      "Ph√≤ng s·∫°ch s·∫Ω, tho·∫£i m√°i. Nh√¢n vi√™n vui v·∫ª th√°i ƒë·ªô ph·ª•c v·ª•c nhi·ªát t√¨nh, th√¢n thi·ªán.\n",
      "\n",
      "Example 358:\n",
      "Ti·ªán nghi ƒë·∫ßy ƒë·ªß tho·∫£i m√°i, nh√¢n vi√™n nhi·ªát t√¨nh v√† r·∫•t th√¢n thi·ªán. Tuy kh√¥ng ph·∫£i kh√°ch s·∫°n t·ªët nh·∫•t nh∆∞ng th√°i ƒë·ªô ph·ª•c v·ª• l√†m m√¨nh r·∫•t ·∫•n t∆∞·ª£ng v√† ƒë·ªÉ l·∫°i d·∫•u ·∫•n t·ªët ·ªü ƒë√¢y. Nh∆∞·ª£c ƒëi·ªÉm h∆°i √≠t ph√≤ng v√† t·∫ßng, h∆°n n·ªØa kh√°ch s·∫°n n·∫±m trong con ƒë∆∞·ªùng m·ªõi x√¢y n√™n di chuy·ªÉn hay g·ªçi xe h·∫ßu nh∆∞ ƒë·ªÅu ph·∫£i ra tr·ª•c ƒë∆∞·ªùng ch√≠nh. View kh√¥ng ƒë·∫πp l·∫Øm\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    N·∫øu d√πng TD-IDF th√¨ s·∫Ω ch·ªâ ch·ªçn d·ª±a v√†o t·∫ßn su·∫•t t·ª´, kh√¥ng hi·ªÉu nghƒ©a c·ªßa c√¢u -> b·ªã tr√πng l·∫∑p\n",
    "    Semantic grouping ƒë·ªÉ ch·ªçn example ƒëa d·∫°ng v·ªÅ n·ªôi dung, kh√¥ng ch·ªâ v·ªÅ c√°c t·ª´\n",
    "'''\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(test_df['Review'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# KMeans clustering (70% ƒëa d·∫°ng, 30% kh√≥)\n",
    "n_total = 5\n",
    "n_diverse = int(n_total * 0.7)\n",
    "n_hard = n_total - n_diverse\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_diverse, random_state=42, n_init=10)\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# L·∫•y diverse example g·∫ßn cluster centroid\n",
    "closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings)\n",
    "diverse_examples = test_df.iloc[closest]\n",
    "\n",
    "print(f\"Selected {len(diverse_examples)} diverse examples:\")\n",
    "for i, row in diverse_examples.iterrows():\n",
    "    print(f\"\\nExample {i}:\\n{row['Review']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "770ce84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19/19 [00:04<00:00,  4.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def select_few_shot_examples(df, text_column, n_total=5, model_name='all-MiniLM-L6-v2', random_state=42):\n",
    "    n_diverse = int(n_total * 0.7)\n",
    "    n_hard = n_total - n_diverse\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(df[text_column].tolist(), show_progress_bar=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_diverse, random_state=random_state, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings)\n",
    "    diverse_df = df.iloc[closest]\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    remaining_indices = list(set(all_indices) - set(closest))\n",
    "    hard_df = df.iloc[remaining_indices].sample(n=n_hard, random_state=random_state)\n",
    "\n",
    "    return diverse_df.reset_index(drop=True), hard_df.reset_index(drop=True)\n",
    "\n",
    "# SET-UP FEW-SHOT EXAMPLES\n",
    "diverse, hard = select_few_shot_examples(test_df, text_column='Review', n_total=10)\n",
    "\n",
    "few_shot_json =  transform_aspect_sentiment(diverse, 0, 100) + transform_aspect_sentiment(hard, 0, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a824d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0',\n",
       "  'text': 'Nh√¢n vi√™n th√¢n thi·ªán, nhi·ªát t√¨nh Nh√† t·∫Øm nh·ªè, ti·ªán nghi ch∆∞a ngƒÉn n·∫Øp, ph√≤ng ·ªëc b√≠.',\n",
       "  'sentiments': [{'aspect': 'ROOMS#COMFORT', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOM_AMENITIES#DESIGN&FEATURES', 'sentiment': 'negative'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '1',\n",
       "  'text': 'Ph√≤ng s·∫°ch s·∫Ω, ƒëang khuy·∫øn m·∫°i n√™n gi√° r·∫ª. S√°ng s·ªõm v√† t·ªëi r·∫•t ·∫ßm ƒ©, v√¨ Kh√°ch s·∫°n v√† nh√† b√™n c·∫°nh ƒëang x√¢y d·ª±ng, ch√≠nh v√¨ v·∫≠y n√™n gi√° ph√≤ng ƒëang gi·∫£m gi√°. L·ªÖ t√¢n ƒë·ª©ng tu·ªïi l·ªãch s·ª± nhi·ªát t√¨nh. L·ªÖ t√¢n tr·∫ª tu·ªïi ban ƒë·∫ßu kh√¥ng cho kh√°ch ƒë·∫∑t ƒë·ªì g·ª≠i trong ph√≤ng g·ª≠i ƒë·ªì, l√Ω do l√† s·ª£ nh·∫ßm l·∫´n v·ªõi ƒë·ªì c·ªßa kh√°ch kh√°c trong ph√≤ng, l√Ω do h·∫øt s·ª©c v√¥ l√Ω, ph·∫£i ƒë·∫øn khi kh√°ch ko ƒë·ªìng √Ω ƒë·ªÉ ngo√†i g√≥c s·∫£nh v√¨ c√≥ m√°y ·∫£nh l·ªÖ t√¢n m·ªõi m·ªü ph√≤ng g·ª≠i ƒë·ªì. V·ªã tr√≠ xa, xung quanh √≠t c·ª≠a h√†ng d·ªãch v·ª•, b√π l·∫°i xe m√°y c√≥ th·ªÉ thu√™ ngay t·∫°i kh√°ch s·∫°n',\n",
       "  'sentiments': [{'aspect': 'FACILITIES#DESIGN&FEATURES',\n",
       "    'sentiment': 'positive'},\n",
       "   {'aspect': 'HOTEL#COMFORT', 'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#MISCELLANEOUS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'LOCATION#GENERAL', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#PRICES', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'neutral'}]},\n",
       " {'id': '2',\n",
       "  'text': 'Ph√≤ng s·∫°ch s·∫Ω, ƒë·∫ßy ƒë·ªß ti·ªán nghi. Qu√° h·ª£p l√Ω v·ªõi m·ª©c gi√° ph√≤ng. G·∫ßn bi·ªÉn, thu·∫≠n ti·ªán ƒëi t·ªõi c√°c ƒë·ªãa ƒëi·ªÉm. Nh√¢n vi√™n kh√°ch s·∫°n nhi·ªát t√¨nh, d·ªÖ m·∫øn. Wifi h∆°i y·∫øu, c·∫ßn khuy·∫øch t√°n wifi v√† c√°ch √¢m ch∆∞a ƒë∆∞·ª£c t·ªët l·∫Øm. Tuy nhi√™n ƒëi·ªÅu ƒë√≥ kh√¥ng qu√° ·∫£nh h∆∞·ªüng, n·∫øu ƒë∆∞·ª£c kh·∫Øc ph·ª•c s·∫Ω ho√†n h·∫£o h∆°n !',\n",
       "  'sentiments': [{'aspect': 'FACILITIES#QUALITY', 'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'LOCATION#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#DESIGN&FEATURES', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOMS#PRICES', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '3',\n",
       "  'text': 'V·ªã tr√≠ t·ªët, ngay c·∫ßu r·ªìng v√† c·∫ßu s√¥ng h√†n , bu·ªïi t·ªëi ra ƒë∆∞·ªùng ng·∫Øm c·∫£nh ƒë·∫πp,g·∫ßn ch·ª£ h√†n thu·∫≠n ti·ªán ƒÉn u·ªëng.nh√¢n vi√™n th√¢n thi·ªán v√† nhi·ªát t√¨nh B·ªØa ƒÉn s√°ng ko ƒë∆∞·ª£c ngon mi·ªáng,√≠t m√≥n,kh√°ch s·∫°n h∆°i xu·ªëng c·∫•p so v·ªõi l·∫ßn ngh·ªâ tr∆∞·ªõc, m·ªÅn r√°ch v√† kh√¥ng ƒë∆∞·ª£c s·∫°ch, n∆∞·ªõc n√≥ng l√∫c ƒë∆∞·ª£c l√∫c kh√¥ng.',\n",
       "  'sentiments': [{'aspect': 'FOOD&DRINKS#QUALITY', 'sentiment': 'negative'},\n",
       "   {'aspect': 'FOOD&DRINKS#STYLE&OPTIONS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#QUALITY', 'sentiment': 'negative'},\n",
       "   {'aspect': 'LOCATION#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#CLEANLINESS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOM_AMENITIES#DESIGN&FEATURES', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOM_AMENITIES#QUALITY', 'sentiment': 'negative'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '4',\n",
       "  'text': 'Ph√≤ng s·∫°ch s·∫Ω, tho·∫£i m√°i. Nh√¢n vi√™n vui v·∫ª th√°i ƒë·ªô ph·ª•c v·ª•c nhi·ªát t√¨nh, th√¢n thi·ªán.',\n",
       "  'sentiments': [{'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#COMFORT', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '5',\n",
       "  'text': 'B·ªØa s√°ng ngon nh∆∞ng kh√° √≠t l·ª±a ch·ªçn, kh√°ch s·∫°n ƒë·∫πp, ph√≤ng g·ªçn g√†ng, d·ªçn ph√≤ng m·ªói ng√†y, nh√¢n vi√™n th√¢n thi·ªán',\n",
       "  'sentiments': [{'aspect': 'FOOD&DRINKS#QUALITY', 'sentiment': 'positive'},\n",
       "   {'aspect': 'FOOD&DRINKS#STYLE&OPTIONS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#DESIGN&FEATURES', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#DESIGN&FEATURES', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '6',\n",
       "  'text': 'Ph√≤ng ƒë·∫πp v√† s·∫°ch s·∫Ω, nh√¢n vi√™n ph·ª•c v·ª• r·∫•t nhi·ªát t√¨nh, h·ªì b∆°i v√† minibar tr√™n s√¢n th∆∞·ª£ng r·∫•t ƒë·∫πp nh∆∞ng sau 22h kh√¥ng ƒë∆∞·ª£c ng·∫Øm c·∫£nh n·ªØa, h∆°i ti·∫øc. ƒê√°nh gi√° chung: r·∫•t t·ªët, s·∫Ω quay l·∫°i v√†o nh·ªØng l·∫ßn sau :)',\n",
       "  'sentiments': [{'aspect': 'FACILITIES#DESIGN&FEATURES',\n",
       "    'sentiment': 'positive'},\n",
       "   {'aspect': 'FACILITIES#MISCELLANEOUS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOMS#DESIGN&FEATURES', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '0',\n",
       "  'text': 'T·ª´ kh√°ch s·∫°n ra t·ªõi bi·ªÉn kh√¥ng qu√° xa. Ph·ª•c v·ª• kh√°ch s·∫°n r·∫•t t·ªët. C√°c qu√°n ƒÉn r·∫•t g·∫ßn v√† ngon ƒêi t·∫Øm kh√¥ng n√™n mang theo d√©p v√¨ c√≥ th·ªÉ b·ªã m·∫•t',\n",
       "  'sentiments': [{'aspect': 'FACILITIES#MISCELLANEOUS',\n",
       "    'sentiment': 'negative'},\n",
       "   {'aspect': 'HOTEL#MISCELLANEOUS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'LOCATION#GENERAL', 'sentiment': 'positive'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]},\n",
       " {'id': '1',\n",
       "  'text': 'Gi∆∞·ªùng r·∫•t tho·∫£i m√°i, ph√≤ng ·ªëc ti·ªán nghi',\n",
       "  'sentiments': [{'aspect': 'ROOMS#COMFORT', 'sentiment': 'positive'},\n",
       "   {'aspect': 'ROOM_AMENITIES#COMFORT', 'sentiment': 'positive'}]},\n",
       " {'id': '2',\n",
       "  'text': 'Nh√¢n vi√™n r·∫•t nhi·ªát t√¨nh, d·ªÖ th∆∞∆°ngüôÇ Trong ph√≤ng ng·ªß, tr√™n gi∆∞·ªùng c√≥ b·ª•i, m·ªÅn v√† g·ªëi c√≥ m√πi .... , ph√≤ng c√°ch √¢m h∆°i t·ªá, r·∫•t ·ªìn',\n",
       "  'sentiments': [{'aspect': 'ROOMS#CLEANLINESS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOMS#COMFORT', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOMS#DESIGN&FEATURES', 'sentiment': 'negative'},\n",
       "   {'aspect': 'ROOM_AMENITIES#CLEANLINESS', 'sentiment': 'negative'},\n",
       "   {'aspect': 'SERVICE#GENERAL', 'sentiment': 'positive'}]}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_json[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3958de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_aspect_sentiment(ground_truth, predictions):\n",
    "    # Chu·∫©n h√≥a d·ªØ li·ªáu th√†nh list c√°c tuple ƒë·ªÉ so s√°nh\n",
    "    true_aspects = []\n",
    "    pred_aspects = []\n",
    "\n",
    "    true_aspect_sentiments = []\n",
    "    pred_aspect_sentiments = []\n",
    "\n",
    "    for gt_entry, pred_entry in zip(ground_truth, predictions):\n",
    "        # ground truth: list of sentiments\n",
    "        gt_sents = gt_entry['sentiments']\n",
    "        gt_aspect_set = set()\n",
    "        gt_aspect_sentiment_set = set()\n",
    "\n",
    "        for item in gt_sents:\n",
    "            gt_aspect_set.add(item['aspect'])\n",
    "            gt_aspect_sentiment_set.add((item['aspect'], item['sentiment']))\n",
    "\n",
    "        true_aspects.append(gt_aspect_set)\n",
    "        true_aspect_sentiments.append(gt_aspect_sentiment_set)\n",
    "\n",
    "        # prediction: list of results\n",
    "        pred_sents = pred_entry['results']\n",
    "        pred_aspect_set = set()\n",
    "        pred_aspect_sentiment_set = set()\n",
    "\n",
    "        for item in pred_sents:\n",
    "            pred_aspect_set.add(item['aspect'])\n",
    "            pred_aspect_sentiment_set.add((item['aspect'], item['sentiment']))\n",
    "\n",
    "        pred_aspects.append(pred_aspect_set)\n",
    "        pred_aspect_sentiments.append(pred_aspect_sentiment_set)\n",
    "\n",
    "    # T√≠nh theo micro-F1 (g·ªôp h·∫øt l·∫°i)\n",
    "    all_true_aspects = set.union(*true_aspects) if true_aspects else set()\n",
    "    all_pred_aspects = set.union(*pred_aspects) if pred_aspects else set()\n",
    "\n",
    "    tp_aspect = sum(len(gt & pred) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "    fp_aspect = sum(len(pred - gt) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "    fn_aspect = sum(len(gt - pred) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "\n",
    "    precision_aspect = tp_aspect / (tp_aspect + fp_aspect + 1e-8)\n",
    "    recall_aspect = tp_aspect / (tp_aspect + fn_aspect + 1e-8)\n",
    "    f1_aspect = 2 * precision_aspect * recall_aspect / (precision_aspect + recall_aspect + 1e-8)\n",
    "\n",
    "    # T√≠nh cho sentiment classification\n",
    "    tp_sentiment = sum(len(gt & pred) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "    fp_sentiment = sum(len(pred - gt) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "    fn_sentiment = sum(len(gt - pred) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "\n",
    "    precision_sentiment = tp_sentiment / (tp_sentiment + fp_sentiment + 1e-8)\n",
    "    recall_sentiment = tp_sentiment / (tp_sentiment + fn_sentiment + 1e-8)\n",
    "    f1_sentiment = 2 * precision_sentiment * recall_sentiment / (precision_sentiment + recall_sentiment + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"Aspect Detection F1\": f1_aspect,\n",
    "        \"Sentiment Classification F1\": f1_sentiment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59cf8edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [04:05<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup Grok API\n",
    "def call_grok_api(prompt):\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that extracts aspects and their sentiments from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_clustering_prompt(text, few_shot_examples):\n",
    "    prompt = \"You are an AI assistant that extracts aspects and their sentiments from text.\\n\\n\"\n",
    "    \n",
    "    # Th√™m few-shot examples\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Extract aspects and sentiments from the following review:\\n{ex['text']}\\n\"\n",
    "        prompt += f\"Result: {json.dumps({'results': ex['sentiments']}, ensure_ascii=False)}\\n\\n\"\n",
    "    \n",
    "    # Th√™m c√¢u h·ªèi hi·ªán t·∫°i\n",
    "    prompt += f\"Extract aspects and sentiments from the following review:\\n{text}\\n\"\n",
    "    prompt += f\"Available aspects: {', '.join(aspects)}\\n\"\n",
    "    prompt += \"Available sentiments: positive, negative, neutral\\n\"\n",
    "    prompt += \"Return ONLY a valid JSON object in this exact format:\\n\"\n",
    "    prompt += '{\"results\": [{\"aspect\": \"aspect_name\", \"sentiment\": \"sentiment_value\"}]}'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_with_grok_clustering(text, few_shot_examples):\n",
    "    prompt = create_clustering_prompt(text, few_shot_examples)\n",
    "    response_text = call_grok_api(prompt)\n",
    "    \n",
    "    if response_text:\n",
    "        try:\n",
    "            # T√¨m JSON trong response\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response_text[start_idx:end_idx]\n",
    "                parsed_output = json.loads(json_str)\n",
    "                return parsed_output\n",
    "            else:\n",
    "                return {\"results\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON Parse Error: {response_text}\")\n",
    "            return {\"results\": []}\n",
    "    else:\n",
    "        return {\"results\": []}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "for data in tqdm(test_json):\n",
    "    prediction = extract_with_grok_clustering(data['text'], few_shot_json)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "722d5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect Detection F1': 0.7214285664238946, 'Sentiment Classification F1': 0.6571428521397108}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_aspect_sentiment(test_json, predictions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11a75653",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = path.join(RESULT_DIR, 'ViABSA_BP_Few-Hotel-Grok.json')\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64b3e0",
   "metadata": {},
   "source": [
    "k = 1 v·ªõi v√≠ d·ª• g·∫ßn t√¢m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "016c0d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 9/100 [00:22<05:11,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parse Error: {\" there! I'm happy to help with extracting aspects and sentiments from the provided review. Based on the text \"R·ªông r√£i, s·∫°ch s·∫Ω. C√≥ ch·ªó trong ph√≤ng kh√¥ng b·∫Øt ƒë∆∞·ª£c wifi\" and the available aspects and sentiments, here's the result in the requested JSON format:\n",
      "\n",
      "{\"results\": [{\"aspect\": \"ROOMS#COMFORT\", \"sentiment\": \"positive\"}, {\"aspect\": \"ROOMS#CLEANLINESS\", \"sentiment\": \"positive\"}, {\"aspect\": \"ROOM_AMENITIES#QUALITY\", \"sentiment\": \"negative\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [05:57<00:00,  3.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup Grok API\n",
    "def call_grok_api(prompt):\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that extracts aspects and their sentiments from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_clustering_prompt(text, few_shot_examples):\n",
    "    prompt = \"You are an AI assistant that extracts aspects and their sentiments from text.\\n\\n\"\n",
    "    \n",
    "    # Th√™m few-shot examples\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Extract aspects and sentiments from the following review:\\n{ex['text']}\\n\"\n",
    "        prompt += f\"Result: {json.dumps({'results': ex['sentiments']}, ensure_ascii=False)}\\n\\n\"\n",
    "    \n",
    "    # Th√™m c√¢u h·ªèi hi·ªán t·∫°i\n",
    "    prompt += f\"Extract aspects and sentiments from the following review:\\n{text}\\n\"\n",
    "    prompt += f\"Available aspects: {', '.join(aspects)}\\n\"\n",
    "    prompt += \"Available sentiments: positive, negative, neutral\\n\"\n",
    "    prompt += \"Return ONLY a valid JSON object in this exact format:\\n\"\n",
    "    prompt += '{\"results\": [{\"aspect\": \"aspect_name\", \"sentiment\": \"sentiment_value\"}]}'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_with_grok_clustering(text, few_shot_examples):\n",
    "    prompt = create_clustering_prompt(text, few_shot_examples)\n",
    "    response_text = call_grok_api(prompt)\n",
    "    \n",
    "    if response_text:\n",
    "        try:\n",
    "            # T√¨m JSON trong response\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response_text[start_idx:end_idx]\n",
    "                parsed_output = json.loads(json_str)\n",
    "                return parsed_output\n",
    "            else:\n",
    "                return {\"results\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON Parse Error: {response_text}\")\n",
    "            return {\"results\": []}\n",
    "    else:\n",
    "        return {\"results\": []}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "for data in tqdm(test_json):\n",
    "    prediction = extract_with_grok_clustering(data['text'], few_shot_json)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1a01ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect Detection F1': 0.709219853148316, 'Sentiment Classification F1': 0.6359338011405922}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_aspect_sentiment(test_json, predictions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08df29da",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = path.join(RESULT_DIR, 'ViABSA_BP_Few-Hotel-Grok-N1-c.json')\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e32251",
   "metadata": {},
   "source": [
    "k = 1 v·ªõi v√≠ d·ª• xa t√¢m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0b7c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [03:11<01:55,  4.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parse Error: {\" {\"results\": [\n",
      "    {\"aspect\": \"LOCATION#GENERAL\", \"sentiment\": \"positive\"},\n",
      "    {\"aspect\": \"SERVICE#GENERAL\", \"sentiment\": \"positive\"},\n",
      "    {\"aspect\": \"FOOD&DRINKS#QUALITY\", \"sentiment\": \"negative\"},\n",
      "    {\"aspect\": \"FOOD&DRINKS#STYLE&OPTIONS\", \"sentiment\": \"negative\"},\n",
      "    {\"aspect\": \"HOTEL#QUALITY\", \"sentiment\": \"negative\"},\n",
      "    {\"aspect\": \"ROOM_AMENITIES#CLEANLINESS\", \"sentiment\": \"negative\"},\n",
      "    {\"aspect\": \"ROOM_AMENITIES#QUALITY\", \"sentiment\": \"negative\"}\n",
      "  ]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [06:03<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup Grok API\n",
    "def call_grok_api(prompt):\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that extracts aspects and their sentiments from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_clustering_prompt(text, few_shot_examples):\n",
    "    prompt = \"You are an AI assistant that extracts aspects and their sentiments from text.\\n\\n\"\n",
    "    \n",
    "    # Th√™m few-shot examples\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Extract aspects and sentiments from the following review:\\n{ex['text']}\\n\"\n",
    "        prompt += f\"Result: {json.dumps({'results': ex['sentiments']}, ensure_ascii=False)}\\n\\n\"\n",
    "    \n",
    "    # Th√™m c√¢u h·ªèi hi·ªán t·∫°i\n",
    "    prompt += f\"Extract aspects and sentiments from the following review:\\n{text}\\n\"\n",
    "    prompt += f\"Available aspects: {', '.join(aspects)}\\n\"\n",
    "    prompt += \"Available sentiments: positive, negative, neutral\\n\"\n",
    "    prompt += \"Return ONLY a valid JSON object in this exact format:\\n\"\n",
    "    prompt += '{\"results\": [{\"aspect\": \"aspect_name\", \"sentiment\": \"sentiment_value\"}]}'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_with_grok_clustering(text, few_shot_examples):\n",
    "    prompt = create_clustering_prompt(text, few_shot_examples)\n",
    "    response_text = call_grok_api(prompt)\n",
    "    \n",
    "    if response_text:\n",
    "        try:\n",
    "            # T√¨m JSON trong response\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response_text[start_idx:end_idx]\n",
    "                parsed_output = json.loads(json_str)\n",
    "                return parsed_output\n",
    "            else:\n",
    "                return {\"results\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON Parse Error: {response_text}\")\n",
    "            return {\"results\": []}\n",
    "    else:\n",
    "        return {\"results\": []}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "for data in tqdm(test_json):\n",
    "    prediction = extract_with_grok_clustering(data['text'], few_shot_json)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21d91e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect Detection F1': 0.7570754666882956, 'Sentiment Classification F1': 0.6910377308407963}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_aspect_sentiment(test_json, predictions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822458f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = path.join(RESULT_DIR, 'ViABSA_BP_Few-Hotel-Grok-N1-f.json')\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9234a",
   "metadata": {},
   "source": [
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccaa8dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [05:01<00:13,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parse Error: {\" there! I'm happy to help with extracting aspects and sentiments from the provided review. Here's the result in the exact JSON format you requested:\n",
      "\n",
      "{\"results\": [{\"aspect\": \"ROOMS#COMFORT\", \"sentiment\": \"positive\"}, {\"aspect\": \"ROOM_AMENITIES#GENERAL\", \"sentiment\": \"positive\"}, {\"aspect\": \"FOOD&DRINKS#STYLE&OPTIONS\", \"sentiment\": \"negative\"}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [05:17<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup Grok API\n",
    "def call_grok_api(prompt):\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that extracts aspects and their sentiments from text.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_clustering_prompt(text, few_shot_examples):\n",
    "    prompt = \"You are an AI assistant that extracts aspects and their sentiments from text.\\n\\n\"\n",
    "    \n",
    "    # Th√™m few-shot examples\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Extract aspects and sentiments from the following review:\\n{ex['text']}\\n\"\n",
    "        prompt += f\"Result: {json.dumps({'results': ex['sentiments']}, ensure_ascii=False)}\\n\\n\"\n",
    "    \n",
    "    # Th√™m c√¢u h·ªèi hi·ªán t·∫°i\n",
    "    prompt += f\"Extract aspects and sentiments from the following review:\\n{text}\\n\"\n",
    "    prompt += f\"Available aspects: {', '.join(aspects)}\\n\"\n",
    "    prompt += \"Available sentiments: positive, negative, neutral\\n\"\n",
    "    prompt += \"Return ONLY a valid JSON object in this exact format:\\n\"\n",
    "    prompt += '{\"results\": [{\"aspect\": \"aspect_name\", \"sentiment\": \"sentiment_value\"}]}'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_with_grok_clustering(text, few_shot_examples):\n",
    "    prompt = create_clustering_prompt(text, few_shot_examples)\n",
    "    response_text = call_grok_api(prompt)\n",
    "    \n",
    "    if response_text:\n",
    "        try:\n",
    "            # T√¨m JSON trong response\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response_text[start_idx:end_idx]\n",
    "                parsed_output = json.loads(json_str)\n",
    "                return parsed_output\n",
    "            else:\n",
    "                return {\"results\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON Parse Error: {response_text}\")\n",
    "            return {\"results\": []}\n",
    "    else:\n",
    "        return {\"results\": []}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "for data in tqdm(test_json):\n",
    "    prediction = extract_with_grok_clustering(data['text'], few_shot_json)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aae634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect Detection F1': 0.8624613505235967, 'Sentiment Classification F1': 0.7211177119997286}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_aspect_sentiment(test_json, predictions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3041d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = path.join(RESULT_DIR, 'ViABSA_BP_Few-Hotel-Grok-N10.json')\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABSA_Prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
