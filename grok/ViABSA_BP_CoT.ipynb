{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02358bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Python\\ABSA_Prompting\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\Python\\ABSA_Prompting\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# Định nghĩa đường dẫn\n",
    "DATA_DIR = r\"c:\\Users\\Admin\\Python\\ABSA_Prompting\\data\"\n",
    "RESULT_DIR = r\"c:\\Users\\Admin\\Python\\ABSA_Prompting\\results\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5704285a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b6616f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "ViABSA_BP_dir = path.join(DATA_DIR, 'ViABSA_BP')\n",
    "test_file = path.join(ViABSA_BP_dir, 'data_test.csv')\n",
    "test_df = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d1fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_aspect_sentiment(df, start=0, end=None):\n",
    "    aspects = [\n",
    "        \"stayingpower\",\n",
    "        \"texture\",\n",
    "        \"smell\",\n",
    "        \"price\",\n",
    "        \"others\",\n",
    "        \"colour\",\n",
    "        \"shipping\",\n",
    "        \"packing\"\n",
    "    ]\n",
    "\n",
    "    if end is None:\n",
    "        end = len(df)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for idx, row in df.iloc[start:end].iterrows():\n",
    "        entry = {\n",
    "            \"id\": str(idx),\n",
    "            \"text\": row['data'],\n",
    "            \"sentiments\": []\n",
    "        }\n",
    "\n",
    "        for aspect in aspects:\n",
    "            sentiment = row[f\"{aspect}_label\"]\n",
    "            if sentiment == 1:  # chỉ lấy những cái có sentiment\n",
    "                aspect_sentiment_value = row[aspect]\n",
    "                if aspect_sentiment_value != 'none':\n",
    "                    entry[\"sentiments\"].append({\n",
    "                        \"aspect\": aspect,\n",
    "                        \"sentiment\": aspect_sentiment_value\n",
    "                    })\n",
    "                else:\n",
    "                    # nếu cột sentiment text bị none nhưng label == 1 thì có thể log ra kiểm tra\n",
    "                    entry[\"sentiments\"].append({\n",
    "                        \"aspect\": aspect,\n",
    "                        \"sentiment\": \"unknown\"\n",
    "                    })\n",
    "\n",
    "        result.append(entry)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61b9392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0',\n",
       "  'text': 'Hàng đóng gói đẹp và chắc chắn, nhìn rất dễ thương và ưng bụng ạ!',\n",
       "  'sentiments': [{'aspect': 'packing', 'sentiment': 'positive'}]},\n",
       " {'id': '1',\n",
       "  'text': 'Cảm giác son bên trong rất là ít luôn í, đóng gói cẩn thận, dịch nhưng mà giao hàng khá nhanh',\n",
       "  'sentiments': [{'aspect': 'shipping', 'sentiment': 'positive'},\n",
       "   {'aspect': 'packing', 'sentiment': 'positive'}]},\n",
       " {'id': '2',\n",
       "  'text': 'Son siêu đẹp luôn ý, mà y hình mà chụp có thể không giống lắm nhưng nhìn ngoài thì giống nha. Chất son mềm mướt nói chung là rất thíchhhh',\n",
       "  'sentiments': [{'aspect': 'texture', 'sentiment': 'positive'}]},\n",
       " {'id': '3',\n",
       "  'text': 'Siu đẹp luôn \\r\\nShop đóng gói cẩn thận lắm luôn \\r\\nLại thêm cả quà nữa\\r\\nNói chung là thích lắm',\n",
       "  'sentiments': [{'aspect': 'packing', 'sentiment': 'positive'}]},\n",
       " {'id': '4',\n",
       "  'text': 'Aúhihđcyihb gfxxth jj bhgfzđE G GHVHBTCEETXUBBIYCZRZRTCVUUBYRZXGVBIINVUTXZRCTIBONONBUXTZRTVKNNOUVTXEecyuvknibtcrztxuvbubibijvcytdgunonobiyvdtrdfyyghuojpmibvytcdtvunomobuCYYVBIBIYCTXYVIONNIXTXTBUOJMOONBIYCXRTXVYBUINNONIBUCTTXTCYVIBONON',\n",
       "  'sentiments': [{'aspect': 'others', 'sentiment': 'neutral'}]}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SETUP DATA\n",
    "aspects = ['stayingpower', 'texture', 'smell', 'price', 'others', 'colour', 'shipping', 'packing']\n",
    "test_df[aspects] = test_df[aspects].fillna('none')\n",
    "\n",
    "for aspect in aspects:\n",
    "    test_df[aspect + '_label'] = (test_df[aspect] != 'none').astype(int)\n",
    "\n",
    "test_json = transform_aspect_sentiment(test_df, 0, 100)\n",
    "test_json[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9687d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_aspect_sentiment(ground_truth, predictions):\n",
    "    # Chuẩn hóa dữ liệu thành list các tuple để so sánh\n",
    "    true_aspects = []\n",
    "    pred_aspects = []\n",
    "\n",
    "    true_aspect_sentiments = []\n",
    "    pred_aspect_sentiments = []\n",
    "\n",
    "    for gt_entry, pred_entry in zip(ground_truth, predictions):\n",
    "        # ground truth: list of sentiments\n",
    "        gt_sents = gt_entry['sentiments']\n",
    "        gt_aspect_set = set()\n",
    "        gt_aspect_sentiment_set = set()\n",
    "\n",
    "        for item in gt_sents:\n",
    "            gt_aspect_set.add(item['aspect'])\n",
    "            gt_aspect_sentiment_set.add((item['aspect'], item['sentiment']))\n",
    "\n",
    "        true_aspects.append(gt_aspect_set)\n",
    "        true_aspect_sentiments.append(gt_aspect_sentiment_set)\n",
    "\n",
    "        # prediction: list of results\n",
    "        pred_sents = pred_entry['results']\n",
    "        pred_aspect_set = set()\n",
    "        pred_aspect_sentiment_set = set()\n",
    "\n",
    "        for item in pred_sents:\n",
    "            pred_aspect_set.add(item['aspect'])\n",
    "            pred_aspect_sentiment_set.add((item['aspect'], item['sentiment']))\n",
    "\n",
    "        pred_aspects.append(pred_aspect_set)\n",
    "        pred_aspect_sentiments.append(pred_aspect_sentiment_set)\n",
    "\n",
    "    # Tính theo micro-F1 (gộp hết lại)\n",
    "    all_true_aspects = set.union(*true_aspects) if true_aspects else set()\n",
    "    all_pred_aspects = set.union(*pred_aspects) if pred_aspects else set()\n",
    "\n",
    "    tp_aspect = sum(len(gt & pred) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "    fp_aspect = sum(len(pred - gt) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "    fn_aspect = sum(len(gt - pred) for gt, pred in zip(true_aspects, pred_aspects))\n",
    "\n",
    "    precision_aspect = tp_aspect / (tp_aspect + fp_aspect + 1e-8)\n",
    "    recall_aspect = tp_aspect / (tp_aspect + fn_aspect + 1e-8)\n",
    "    f1_aspect = 2 * precision_aspect * recall_aspect / (precision_aspect + recall_aspect + 1e-8)\n",
    "\n",
    "    # Tính cho sentiment classification\n",
    "    tp_sentiment = sum(len(gt & pred) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "    fp_sentiment = sum(len(pred - gt) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "    fn_sentiment = sum(len(gt - pred) for gt, pred in zip(true_aspect_sentiments, pred_aspect_sentiments))\n",
    "\n",
    "    precision_sentiment = tp_sentiment / (tp_sentiment + fp_sentiment + 1e-8)\n",
    "    recall_sentiment = tp_sentiment / (tp_sentiment + fn_sentiment + 1e-8)\n",
    "    f1_sentiment = 2 * precision_sentiment * recall_sentiment / (precision_sentiment + recall_sentiment + 1e-8)\n",
    "\n",
    "    return {\n",
    "        \"Aspect Detection F1\": f1_aspect,\n",
    "        \"Sentiment Classification F1\": f1_sentiment\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8c0aefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:10<00:00,  4.80it/s]\n"
     ]
    }
   ],
   "source": [
    "def select_few_shot_examples(df, text_column, n_total=10, model_name='all-MiniLM-L6-v2', random_state=42):\n",
    "    n_diverse = int(n_total * 0.7)\n",
    "    n_hard = n_total - n_diverse\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(df[text_column].tolist(), show_progress_bar=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_diverse, random_state=random_state, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    closest, _ = pairwise_distances_argmin_min(kmeans.cluster_centers_, embeddings)\n",
    "    diverse_df = df.iloc[closest]\n",
    "\n",
    "    all_indices = np.arange(len(df))\n",
    "    remaining_indices = list(set(all_indices) - set(closest))\n",
    "    hard_df = df.iloc[remaining_indices].sample(n=n_hard, random_state=random_state)\n",
    "\n",
    "    return diverse_df.reset_index(drop=True), hard_df.reset_index(drop=True)\n",
    "\n",
    "# SET-UP FEW-SHOT EXAMPLES\n",
    "diverse, hard = select_few_shot_examples(test_df, text_column='data', n_total=5)\n",
    "\n",
    "few_shot_json =  transform_aspect_sentiment(diverse, 0, 100) + transform_aspect_sentiment(hard, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d624fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:06<00:00,  1.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# Setup Grok API\n",
    "def call_grok_api(prompt):\n",
    "    \"\"\"\n",
    "    Gọi Grok API sử dụng requests\n",
    "    \"\"\"\n",
    "    url = \"https://api.x.ai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.getenv('GROK_API_KEY')}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": \"grok-3\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI assistant that extracts aspects and their sentiments from text. Think step by step exactly.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        else:\n",
    "            print(f\"API Error: {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_few_shot_prompt(text, few_shot_examples):\n",
    "    prompt = \"You are an AI assistant that extracts aspects and their sentiments from text. Think step by step exactly.\\n\\n\"\n",
    "    \n",
    "    # Thêm few-shot examples\n",
    "    for ex in few_shot_examples:\n",
    "        prompt += f\"Extract aspects and sentiments from the following review:\\n{ex['text']}\\n\"\n",
    "        prompt += f\"Result: {json.dumps({'results': ex['sentiments']}, ensure_ascii=False)}\\n\\n\"\n",
    "    \n",
    "    # Thêm câu hỏi hiện tại\n",
    "    prompt += f\"Extract aspects and sentiments from the following review:\\n{text}\\n\"\n",
    "    prompt += f\"Available aspects: {', '.join(aspects)}\\n\"\n",
    "    prompt += \"Available sentiments: positive, negative, neutral\\n\"\n",
    "    prompt += \"Return ONLY a valid JSON object in this exact format:\\n\"\n",
    "    prompt += '{\"results\": [{\"aspect\": \"aspect_name\", \"sentiment\": \"sentiment_value\"}]}'\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def extract_with_grok_cot(text, few_shot_examples):\n",
    "    prompt = create_few_shot_prompt(text, few_shot_examples)\n",
    "    response_text = call_grok_api(prompt)\n",
    "    \n",
    "    if response_text:\n",
    "        try:\n",
    "            # Tìm JSON trong response\n",
    "            start_idx = response_text.find('{')\n",
    "            end_idx = response_text.rfind('}') + 1\n",
    "            \n",
    "            if start_idx != -1 and end_idx != -1:\n",
    "                json_str = response_text[start_idx:end_idx]\n",
    "                parsed_output = json.loads(json_str)\n",
    "                return parsed_output\n",
    "            else:\n",
    "                return {\"results\": []}\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"JSON Parse Error: {response_text}\")\n",
    "            return {\"results\": []}\n",
    "    else:\n",
    "        return {\"results\": []}\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# Dự đoán\n",
    "for data in tqdm(test_json):\n",
    "    prediction = extract_with_grok_cot(data['text'], few_shot_json)\n",
    "    predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b42c763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspect Detection F1': 0.902912616341903, 'Sentiment Classification F1': 0.7815533930468235}\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_aspect_sentiment(test_json, predictions)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "360702ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = path.join(RESULT_DIR, 'ViABSA_BP_CoT_Grok.json')\n",
    "with open(result_file, 'w') as f:\n",
    "    json.dump(predictions, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ABSA_Prompting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
